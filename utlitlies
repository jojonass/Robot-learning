import numpy as np
from collections import defaultdict
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback

class SuccessCallback(BaseCallback):
    def __init__(self, log_freq=10000, verbose=0):
        super().__init__(verbose)
        self.log_freq = log_freq
        self.task_successes = defaultdict(list)
        self.env_current_success = defaultdict(float)

    def _on_step(self) -> bool:
        infos = self.locals.get("infos")
        dones = self.locals.get("dones")

        if infos is None or dones is None:
            return True

        for i, info in enumerate(infos):
            # Works for MT1/3/10 as long as 'success' is in info
            if "success" in info:
                self.env_current_success[i] = max(self.env_current_success[i], info["success"])

        for i, done in enumerate(dones):
            if done:
                info = infos[i]
                # Default to 'task' if task_name isn't provided (for simple MT1 setups)
                task = info.get("task_name", "single_task")
                self.task_successes[task].append(self.env_current_success[i])
                self.env_current_success[i] = 0.0

        if self.num_timesteps % self.log_freq == 0:
            self._log_success_rates()
        return True

    def _log_success_rates(self):
        all_rates = []
        for task, successes in self.task_successes.items():
            if successes:
                rate = np.mean(successes[-100:])
                self.logger.record(f"success/{task}", rate)
                all_rates.append(rate)
        
        # Only log a 'mean' if we are doing Multi-Task (MT3, MT10, etc.)
        if len(all_rates) > 1:
            self.logger.record("success/mean_combined", np.mean(all_rates))


class EvalSuccessCallback(EvalCallback):
    def _on_step(self) -> bool:
        result = super()._on_step()
        if self.n_calls % self.eval_freq == 0:
            self._run_eval_success()
        return result

    def _run_eval_success(self):
        task_success = defaultdict(list)
        for _ in range(self.n_eval_episodes):
            obs = self.eval_env.reset()
            done = False
            last_info = {}

            while not done:
                action, _ = self.model.predict(obs, deterministic=True)
                obs, reward, done_array, info_array = self.eval_env.step(action)

                done = done_array[0]
                info = info_array[0]
                last_info = info

                if info.get("success", 0) > 0:
                    task_name = info.get("task_name", "single_task")
                    task_success[task_name].append(1.0)
                    done = True 
                    break

            if not last_info.get("success", 0) > 0:
                task_name = last_info.get("task_name", "single_task")
                task_success[task_name].append(0.0)

        means = []
        for task, values in task_success.items():
            mean = np.mean(values)
            self.logger.record(f"eval_success/{task}", mean)
            means.append(mean)

        # Log combined mean only for MT3/MT10
        if len(means) > 1:
            self.logger.record("eval_success/mean_combined", np.mean(means))

        self.logger.dump(self.num_timesteps)
